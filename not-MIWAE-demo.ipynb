{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "How to work on a Github repo with Colab: [stackoverflow](https://stackoverflow.com/questions/52681405how-can-i-import-custom-modules-from-a-github-repository-in-google-colab)"
      ],
      "metadata": {
        "id": "8MXCVyQsvPMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run if destination doesn't already exists\n",
        "!git clone https://github.com/VincentGefflaut/notMIWAE-project.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cm6cAN7vB6t",
        "outputId": "1a3983ee-7edb-40d7-d76c-b0a3763a2b7f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'notMIWAE-project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Always run before working in Colab\n",
        "import sys\n",
        "sys.path.insert(0,'/content/notMIWAE-project')\n",
        "!cd notMIWAE-project; git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrTBJOrmwIvG",
        "outputId": "464cc836-901c-4dd5-90d4-92fbdfad756c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add path_prefix before all paths\n",
        "path_prefix = '/content/notMIWAE-project/'"
      ],
      "metadata": {
        "id": "wnBSqTnlxHSR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJVf24m6s1Ku"
      },
      "source": [
        "# not-MIWAE: Deep Generative Modelling with Missing not at Random Data\n",
        "This notebook illustrates how to fit a *deep latent variable model* to data affected by a missing process which depends on the missing data itself, i.e. *missing not at random*.\n",
        "\n",
        "We fit a linear PPCA-like model to a relatively small UCI dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SO_lapTs1Kw"
      },
      "source": [
        "### Preamble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "N1_pgGKxs1Kz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "import tensorflow_probability as tfp\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# import sys\n",
        "# sys.path.append('./')\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
        "\n",
        "plt.rcParams[\"font.family\"] = \"serif\"\n",
        "plt.rcParams['font.size'] = 15.0\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['savefig.format'] = 'pdf'\n",
        "plt.rcParams['lines.linewidth'] = 2.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29c9e323",
        "outputId": "d4d916b2-632d-41c4-9a1d-1988d1faaa80"
      },
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "    print(\"GPU is available and being used!\")\n",
        "    print(f\"GPU Device: {tf.test.gpu_device_name()}\")\n",
        "else:\n",
        "    print(\"GPU is NOT available. Please check your Colab runtime type (Runtime > Change runtime type > GPU).\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available and being used!\n",
            "GPU Device: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbFrR_ons1K0"
      },
      "source": [
        "### Load data\n",
        "Here we use the white-wine dataset from the UCI database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "collapsed": true,
        "id": "mjF4B391s1K6"
      },
      "outputs": [],
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
        "data = np.array(pd.read_csv(url, low_memory=False, sep=';'))\n",
        "# ---- drop the classification attribute\n",
        "data = data[:, :-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6WtGhAjs1K6"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "collapsed": true,
        "id": "-ZXXEJQvs1K7"
      },
      "outputs": [],
      "source": [
        "N, D = data.shape\n",
        "n_latent = D - 1\n",
        "n_hidden = 128\n",
        "n_samples = 20\n",
        "max_iter = 30000\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIYVFUtds1K7"
      },
      "source": [
        "### Standardize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "collapsed": true,
        "id": "BvZVVnb7s1K7"
      },
      "outputs": [],
      "source": [
        "# ---- standardize data\n",
        "data = data - np.mean(data, axis=0)\n",
        "data = data / np.std(data, axis=0)\n",
        "\n",
        "# ---- random permutation\n",
        "p = np.random.permutation(N)\n",
        "data = data[p, :]\n",
        "\n",
        "# ---- we use the full dataset for training here, but you can make a train-val split\n",
        "Xtrain = data.copy()\n",
        "Xval = Xtrain.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfTC_O4gs1K8"
      },
      "source": [
        "### Introduce missing\n",
        "Here we denote\n",
        "- Xnan: data matrix with np.nan as the missing entries\n",
        "- Xz: data matrix with 0 as the missing entries\n",
        "- S: missing mask\n",
        "\n",
        "The missing process depends on the missing data itself:\n",
        "- in half the features, set the feature value to missing when it is higher than the feature mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "collapsed": true,
        "id": "tanHFXQgs1K8"
      },
      "outputs": [],
      "source": [
        "# ---- introduce missing process\n",
        "Xnan = Xtrain.copy()\n",
        "Xz = Xtrain.copy()\n",
        "\n",
        "mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
        "ix_larger_than_mean = Xnan[:, :int(D / 2)] > mean\n",
        "\n",
        "Xnan[:, :int(D / 2)][ix_larger_than_mean] = np.nan\n",
        "Xz[:, :int(D / 2)][ix_larger_than_mean] = 0\n",
        "\n",
        "S = np.array(~np.isnan(Xnan), dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk8c6JKVs1K8"
      },
      "source": [
        "### Build the model\n",
        "The model we are building has a Gaussian prior and a Gaussian observation model,\n",
        "\n",
        "$$ p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} | \\mathbf{0}, \\mathbf{I})$$\n",
        "\n",
        "$$ p(\\mathbf{x} | \\mathbf{z}) = \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_{\\theta}(\\mathbf{z}), \\sigma^2\\mathbf{I})$$\n",
        "\n",
        "$$ p(\\mathbf{x}) = \\int p(\\mathbf{x} | \\mathbf{z})p(\\mathbf{z}) d\\mathbf{z}$$\n",
        "\n",
        "where $\\mathbf{\\mu}_{\\theta}(\\mathbf{z}): \\mathbb{R}^d \\rightarrow \\mathbb{R}^p $ in general is a deep neural net, but in this case is a linear mapping, $\\mathbf{\\mu} = \\mathbf{Wz + b}$.\n",
        "\n",
        "The variational posterior is also Gaussian\n",
        "\n",
        "$$q_{\\gamma}(\\mathbf{z} | \\mathbf{x}) = \\mathcal{N}(\\mathbf{z} | \\mu_{\\gamma}(\\mathbf{x}), \\sigma_{\\gamma}(\\mathbf{x})^2 \\mathbf{I})$$\n",
        "\n",
        "If the missing process is *missing at random*, it is ignorable and the ELBO becomes, as described in [the MIWAE paper](https://arxiv.org/abs/1812.02633)\n",
        "\n",
        "$$ E_{\\mathbf{z}_1...\\mathbf{z}_K} \\left[ \\log \\frac{1}{K}\\sum_{k=1}^K \\frac{p_{\\theta}(\\mathbf{x^o} | \\mathbf{z}_k)p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z}_k | \\mathbf{x^o})} \\right] $$\n",
        "\n",
        "When the missing process is MNAR it is non-ignorable and we need to include the missing model. In this example we include the missing model as a logistic regression in each feature dimension\n",
        "\n",
        "$$ p_{\\phi}(\\mathbf{s} | \\mathbf{x^o, x^m}) = \\text{Bern}(\\mathbf{s} | \\pi_{\\phi}(\\mathbf{x^o, x^m}))$$\n",
        "\n",
        "$$ \\pi_{\\phi, j}(x_j) = \\frac{1}{1 + e^{-\\text{logits}_j}} $$\n",
        "\n",
        "$$ \\text{logits}_j = W_j (x_j - b_j) $$\n",
        "\n",
        "The ELBO in the MNAR case becomes\n",
        "\n",
        "$$ E_{(\\mathbf{z}_1, \\mathbf{x}_1^m)...(\\mathbf{z}_K, \\mathbf{x}_K^m)} \\left[ \\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\phi}(\\mathbf{s} | \\mathbf{x}^o, \\mathbf{x}_k^m) p_{\\theta}(\\mathbf{x}^o | \\mathbf{z}_k) p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z} | \\mathbf{x}^o)} \\right]$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEWT1fZRs1K8"
      },
      "source": [
        "### Inputs\n",
        "Let's first define the inputs of the model\n",
        "- x_pl: data input\n",
        "- s_pl: mask input\n",
        "- n_pl: number of importance samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "RysKX0gws1K8",
        "outputId": "5942afea-3ddf-4b67-ad59-4d06bd648572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating graph...\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating graph...\")\n",
        "tf.reset_default_graph()\n",
        "# ---- input\n",
        "with tf.variable_scope('input'):\n",
        "    x_pl = tf.placeholder(tf.float32, [None, D], 'x_pl')\n",
        "    s_pl = tf.placeholder(tf.float32, [None, D], 's_pl')\n",
        "    n_pl = tf.placeholder(tf.int32, shape=(), name='n_pl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEMQgCRss1K9"
      },
      "source": [
        "the noise variance is learned as a shared parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "IPpEejAms1K9"
      },
      "outputs": [],
      "source": [
        "# ---- parameters\n",
        "with tf.variable_scope('data_process'):\n",
        "    logstd = tf.get_variable('logstd', shape=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gc-zPims1K9"
      },
      "source": [
        "### Encoder\n",
        "The encoder / inference network consists of two hidden layers with 128 units and tanh activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zOsy4YFls1K9"
      },
      "outputs": [],
      "source": [
        "x = keras.layers.Dense(units=n_hidden, activation=tf.nn.tanh, name='l_enc1')(x_pl)\n",
        "x = keras.layers.Dense(units=n_hidden, activation=tf.nn.tanh, name='l_enc2')(x)\n",
        "\n",
        "q_mu = keras.layers.Dense(units=n_latent, activation=None, name='q_mu')(x)\n",
        "\n",
        "q_logstd = keras.layers.Dense(units=n_latent, activation=lambda x: tf.clip_by_value(x, -10, 10),\n",
        "                           name='q_logstd')(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjgzj1_6s1K9"
      },
      "source": [
        "### Variational distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "id": "gei-Q30os1K9"
      },
      "outputs": [],
      "source": [
        "q_z = tfp.distributions.Normal(loc=q_mu, scale=tf.exp(q_logstd))\n",
        "\n",
        "# ---- sample the latent value\n",
        "l_z = q_z.sample(n_pl)                    # shape [n_samples, batch_size, dl]\n",
        "l_z = tf.transpose(l_z, perm=[1, 0, 2])   # shape [batch_size, n_samples, dl]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Xl8gEBs1K-"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "collapsed": true,
        "id": "YpHq46ybs1K-"
      },
      "outputs": [],
      "source": [
        "mu = keras.layers.Dense(units=D, activation=None, name='mu')(l_z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLa0j2eCs1K-"
      },
      "source": [
        "### Observation model / likelihood function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "collapsed": true,
        "id": "fcpHd5fas1K-"
      },
      "outputs": [],
      "source": [
        "p_x_given_z = tfp.distributions.Normal(loc=mu, scale=tf.exp(logstd))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W33DQKNCs1K-"
      },
      "source": [
        "### Missing model\n",
        "- first mix observed data and samples of missing data\n",
        "- feed through missing model\n",
        "- find likelihood of missing model parameters\n",
        "\n",
        "We have to expand the dimensions of x_pl and s_pl, since mu has size [batch, n_samples, D]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "collapsed": true,
        "id": "t8sa4f4ss1K-"
      },
      "outputs": [],
      "source": [
        "l_out_mixed = mu * tf.expand_dims(1 - s_pl, axis=1) + tf.expand_dims(x_pl * s_pl, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": true,
        "id": "UBIQX7uAs1K_"
      },
      "outputs": [],
      "source": [
        "W = tf.get_variable('W', shape=[1, 1, D])\n",
        "W = -tf.nn.softplus(W)\n",
        "b = tf.get_variable('b', shape=[1, 1, D])\n",
        "\n",
        "logits = W * (l_out_mixed - b)\n",
        "\n",
        "p_s_given_x = tfp.distributions.Bernoulli(logits=logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGY2DH-ls1K_"
      },
      "source": [
        "### Evaluating likelihoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "3iyHEhNYs1K_"
      },
      "outputs": [],
      "source": [
        "# ---- evaluate the observed data in p(x|z)\n",
        "log_p_x_given_z = tf.reduce_sum(tf.expand_dims(s_pl, axis=1) *\n",
        "                                p_x_given_z.log_prob(tf.expand_dims(x_pl, axis=1)), axis=-1)  # sum over d-dimension\n",
        "\n",
        "# --- evaluate the z-samples in q(z|x)\n",
        "q_z2 = tfp.distributions.Normal(loc=tf.expand_dims(q_z.loc, axis=1), scale=tf.expand_dims(q_z.scale, axis=1))\n",
        "log_q_z_given_x = tf.reduce_sum(q_z2.log_prob(l_z), axis=-1)\n",
        "\n",
        "# ---- evaluate the z-samples in the prior p(z)\n",
        "prior = tfp.distributions.Normal(loc=0.0, scale=1.0)\n",
        "log_p_z = tf.reduce_sum(prior.log_prob(l_z), axis=-1)\n",
        "\n",
        "# ---- evaluate the mask in p(s|x)\n",
        "log_p_s_given_x = tf.reduce_sum(p_s_given_x.log_prob(tf.expand_dims(s_pl, axis=1)), axis=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aemkkQIks1K_"
      },
      "source": [
        "### Losses for the MIWAE and not-MIWAE respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "collapsed": true,
        "id": "Cqnjl80Bs1K_"
      },
      "outputs": [],
      "source": [
        "lpxz = log_p_x_given_z\n",
        "lpz = log_p_z\n",
        "lqzx = log_q_z_given_x\n",
        "lpsx = log_p_s_given_x\n",
        "\n",
        "# ---- MIWAE\n",
        "# ---- importance weights\n",
        "l_w = lpxz + lpz - lqzx\n",
        "\n",
        "# ---- sum over samples\n",
        "log_sum_w = tf.reduce_logsumexp(l_w, axis=1)\n",
        "\n",
        "# ---- average over samples\n",
        "log_avg_weight = log_sum_w - tf.log(tf.cast(n_pl, tf.float32))\n",
        "\n",
        "# ---- average over minibatch to get the average llh\n",
        "MIWAE = tf.reduce_mean(log_avg_weight, axis=-1)\n",
        "\n",
        "\n",
        "# ---- not-MIWAE\n",
        "# ---- importance weights\n",
        "l_w = lpxz + lpsx + lpz - lqzx\n",
        "\n",
        "# ---- sum over samples\n",
        "log_sum_w = tf.reduce_logsumexp(l_w, axis=1)\n",
        "\n",
        "# ---- average over samples\n",
        "log_avg_weight = log_sum_w - tf.log(tf.cast(n_pl, tf.float32))\n",
        "\n",
        "# ---- average over minibatch to get the average llh\n",
        "notMIWAE = tf.reduce_mean(log_avg_weight, axis=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1ZpYiFVs1LA"
      },
      "source": [
        "### Training stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "collapsed": true,
        "id": "ylOtf5cas1LA"
      },
      "outputs": [],
      "source": [
        "# ---- training stuff\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "global_step = tf.Variable(initial_value=0, trainable=False)\n",
        "optimizer = tf.train.AdamOptimizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGNo-P9rs1LA"
      },
      "source": [
        "### Choose wether you want to train the MIWAE or the notMIWAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "collapsed": true,
        "id": "0Zt-rsR0s1LA"
      },
      "outputs": [],
      "source": [
        "loss = -notMIWAE\n",
        "# loss = -MIWAE\n",
        "\n",
        "tvars = tf.trainable_variables()\n",
        "train_op = optimizer.minimize(loss, global_step=global_step, var_list=tvars)\n",
        "sess.run(tf.global_variables_initializer())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Deju_Ks1LA"
      },
      "source": [
        "### Do the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "MjfX93Ens1LB",
        "outputId": "8faf55ae-ca03-49dc-8182-2ca54414d6b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/30000 updates, 2.69 s, 43.58 train_loss, 42.14 val_loss\n",
            "100/30000 updates, 0.44 s, 24.85 train_loss, 21.77 val_loss\n",
            "200/30000 updates, 0.33 s, 21.21 train_loss, 19.94 val_loss\n",
            "300/30000 updates, 0.34 s, 20.60 train_loss, 18.99 val_loss\n",
            "400/30000 updates, 0.34 s, 19.34 train_loss, 17.97 val_loss\n",
            "500/30000 updates, 0.33 s, 17.08 train_loss, 17.41 val_loss\n",
            "600/30000 updates, 0.35 s, 16.25 train_loss, 16.87 val_loss\n",
            "700/30000 updates, 0.34 s, 17.01 train_loss, 16.36 val_loss\n",
            "800/30000 updates, 0.32 s, 15.40 train_loss, 16.06 val_loss\n",
            "900/30000 updates, 0.34 s, 14.72 train_loss, 15.67 val_loss\n",
            "1000/30000 updates, 0.33 s, 15.81 train_loss, 15.36 val_loss\n",
            "1100/30000 updates, 0.33 s, 14.91 train_loss, 15.07 val_loss\n",
            "1200/30000 updates, 0.37 s, 13.88 train_loss, 14.82 val_loss\n",
            "1300/30000 updates, 0.33 s, 13.66 train_loss, 14.60 val_loss\n",
            "1400/30000 updates, 0.43 s, 13.72 train_loss, 14.41 val_loss\n",
            "1500/30000 updates, 0.57 s, 14.31 train_loss, 14.21 val_loss\n",
            "1600/30000 updates, 0.52 s, 13.16 train_loss, 14.02 val_loss\n",
            "1700/30000 updates, 0.54 s, 14.15 train_loss, 14.05 val_loss\n",
            "1800/30000 updates, 0.46 s, 12.89 train_loss, 13.72 val_loss\n",
            "1900/30000 updates, 0.36 s, 13.66 train_loss, 13.74 val_loss\n",
            "2000/30000 updates, 0.33 s, 12.39 train_loss, 13.46 val_loss\n",
            "2100/30000 updates, 0.34 s, 14.72 train_loss, 13.30 val_loss\n",
            "2200/30000 updates, 0.36 s, 12.42 train_loss, 13.19 val_loss\n",
            "2300/30000 updates, 0.33 s, 11.70 train_loss, 13.11 val_loss\n",
            "2400/30000 updates, 0.33 s, 12.26 train_loss, 12.96 val_loss\n",
            "2500/30000 updates, 0.35 s, 15.48 train_loss, 12.85 val_loss\n",
            "2600/30000 updates, 0.33 s, 12.89 train_loss, 12.83 val_loss\n",
            "2700/30000 updates, 0.34 s, 12.84 train_loss, 12.69 val_loss\n",
            "2800/30000 updates, 0.35 s, 12.44 train_loss, 12.64 val_loss\n",
            "2900/30000 updates, 0.34 s, 12.82 train_loss, 12.57 val_loss\n",
            "3000/30000 updates, 0.33 s, 12.05 train_loss, 12.51 val_loss\n",
            "3100/30000 updates, 0.35 s, 26.26 train_loss, 12.45 val_loss\n",
            "3200/30000 updates, 0.33 s, 11.26 train_loss, 12.40 val_loss\n",
            "3300/30000 updates, 0.33 s, 11.28 train_loss, 12.37 val_loss\n",
            "3400/30000 updates, 0.35 s, 12.02 train_loss, 12.34 val_loss\n",
            "3500/30000 updates, 0.33 s, 13.59 train_loss, 12.31 val_loss\n",
            "3600/30000 updates, 0.34 s, 12.91 train_loss, 12.33 val_loss\n",
            "3700/30000 updates, 0.35 s, 12.54 train_loss, 12.19 val_loss\n",
            "3800/30000 updates, 0.33 s, 11.30 train_loss, 12.16 val_loss\n",
            "3900/30000 updates, 0.33 s, 11.81 train_loss, 12.15 val_loss\n",
            "4000/30000 updates, 0.36 s, 11.19 train_loss, 12.09 val_loss\n",
            "4100/30000 updates, 0.33 s, 11.73 train_loss, 12.07 val_loss\n",
            "4200/30000 updates, 0.33 s, 11.42 train_loss, 12.12 val_loss\n",
            "4300/30000 updates, 0.35 s, 12.13 train_loss, 12.07 val_loss\n",
            "4400/30000 updates, 0.33 s, 11.80 train_loss, 12.03 val_loss\n",
            "4500/30000 updates, 0.33 s, 10.65 train_loss, 12.10 val_loss\n",
            "4600/30000 updates, 0.35 s, 11.86 train_loss, 12.25 val_loss\n",
            "4700/30000 updates, 0.33 s, 10.84 train_loss, 12.01 val_loss\n",
            "4800/30000 updates, 0.54 s, 10.49 train_loss, 11.97 val_loss\n",
            "4900/30000 updates, 0.52 s, 13.20 train_loss, 11.96 val_loss\n",
            "5000/30000 updates, 0.51 s, 11.87 train_loss, 11.99 val_loss\n",
            "5100/30000 updates, 0.55 s, 11.74 train_loss, 11.93 val_loss\n",
            "5200/30000 updates, 0.33 s, 11.18 train_loss, 11.98 val_loss\n",
            "5300/30000 updates, 0.35 s, 11.26 train_loss, 11.89 val_loss\n",
            "5400/30000 updates, 0.33 s, 12.92 train_loss, 11.87 val_loss\n",
            "5500/30000 updates, 0.34 s, 11.36 train_loss, 11.89 val_loss\n",
            "5600/30000 updates, 0.34 s, 12.11 train_loss, 11.87 val_loss\n",
            "5700/30000 updates, 0.34 s, 12.09 train_loss, 11.87 val_loss\n",
            "5800/30000 updates, 0.34 s, 12.35 train_loss, 11.85 val_loss\n",
            "5900/30000 updates, 0.33 s, 11.91 train_loss, 11.86 val_loss\n",
            "6000/30000 updates, 0.35 s, 27.15 train_loss, 11.84 val_loss\n",
            "6100/30000 updates, 0.33 s, 12.21 train_loss, 11.82 val_loss\n",
            "6200/30000 updates, 0.33 s, 11.52 train_loss, 11.79 val_loss\n",
            "6300/30000 updates, 0.35 s, 12.31 train_loss, 11.80 val_loss\n",
            "6400/30000 updates, 0.33 s, 10.68 train_loss, 11.78 val_loss\n",
            "6500/30000 updates, 0.33 s, 12.88 train_loss, 11.87 val_loss\n",
            "6600/30000 updates, 0.35 s, 13.43 train_loss, 11.77 val_loss\n",
            "6700/30000 updates, 0.33 s, 11.77 train_loss, 11.80 val_loss\n",
            "6800/30000 updates, 0.33 s, 12.04 train_loss, 11.75 val_loss\n",
            "6900/30000 updates, 0.36 s, 11.36 train_loss, 11.74 val_loss\n",
            "7000/30000 updates, 0.33 s, 11.59 train_loss, 11.78 val_loss\n",
            "7100/30000 updates, 0.33 s, 10.50 train_loss, 11.79 val_loss\n",
            "7200/30000 updates, 0.34 s, 11.55 train_loss, 11.78 val_loss\n",
            "7300/30000 updates, 0.33 s, 12.45 train_loss, 11.75 val_loss\n",
            "7400/30000 updates, 0.33 s, 9.98 train_loss, 11.73 val_loss\n",
            "7500/30000 updates, 0.35 s, 10.93 train_loss, 11.75 val_loss\n",
            "7600/30000 updates, 0.33 s, 10.98 train_loss, 11.88 val_loss\n",
            "7700/30000 updates, 0.34 s, 11.74 train_loss, 11.99 val_loss\n",
            "7800/30000 updates, 0.35 s, 11.71 train_loss, 11.77 val_loss\n",
            "7900/30000 updates, 0.33 s, 11.67 train_loss, 11.74 val_loss\n",
            "8000/30000 updates, 0.33 s, 12.19 train_loss, 11.71 val_loss\n",
            "8100/30000 updates, 0.45 s, 11.58 train_loss, 11.69 val_loss\n",
            "8200/30000 updates, 0.52 s, 11.09 train_loss, 11.73 val_loss\n",
            "8300/30000 updates, 0.53 s, 11.13 train_loss, 11.71 val_loss\n",
            "8400/30000 updates, 0.55 s, 11.30 train_loss, 11.68 val_loss\n",
            "8500/30000 updates, 0.45 s, 10.56 train_loss, 11.67 val_loss\n",
            "8600/30000 updates, 0.33 s, 11.10 train_loss, 11.69 val_loss\n",
            "8700/30000 updates, 0.33 s, 11.55 train_loss, 11.72 val_loss\n",
            "8800/30000 updates, 0.36 s, 12.61 train_loss, 11.63 val_loss\n",
            "8900/30000 updates, 0.34 s, 10.41 train_loss, 11.66 val_loss\n",
            "9000/30000 updates, 0.33 s, 11.32 train_loss, 11.75 val_loss\n",
            "9100/30000 updates, 0.35 s, 11.26 train_loss, 11.76 val_loss\n",
            "9200/30000 updates, 0.34 s, 11.27 train_loss, 11.68 val_loss\n",
            "9300/30000 updates, 0.34 s, 11.46 train_loss, 11.66 val_loss\n",
            "9400/30000 updates, 0.35 s, 11.85 train_loss, 11.67 val_loss\n",
            "9500/30000 updates, 0.34 s, 10.99 train_loss, 11.63 val_loss\n",
            "9600/30000 updates, 0.33 s, 10.55 train_loss, 11.63 val_loss\n",
            "9700/30000 updates, 0.40 s, 10.94 train_loss, 11.64 val_loss\n",
            "9800/30000 updates, 0.34 s, 11.40 train_loss, 11.61 val_loss\n",
            "9900/30000 updates, 0.34 s, 11.16 train_loss, 11.62 val_loss\n",
            "10000/30000 updates, 0.36 s, 11.63 train_loss, 11.60 val_loss\n",
            "10100/30000 updates, 0.33 s, 10.84 train_loss, 11.61 val_loss\n",
            "10200/30000 updates, 0.33 s, 10.87 train_loss, 11.63 val_loss\n",
            "10300/30000 updates, 0.35 s, 11.90 train_loss, 11.60 val_loss\n",
            "10400/30000 updates, 0.33 s, 11.73 train_loss, 11.62 val_loss\n",
            "10500/30000 updates, 0.33 s, 12.52 train_loss, 11.69 val_loss\n",
            "10600/30000 updates, 0.36 s, 11.48 train_loss, 11.65 val_loss\n",
            "10700/30000 updates, 0.33 s, 11.80 train_loss, 11.66 val_loss\n",
            "10800/30000 updates, 0.33 s, 10.72 train_loss, 11.57 val_loss\n",
            "10900/30000 updates, 0.35 s, 11.49 train_loss, 11.56 val_loss\n",
            "11000/30000 updates, 0.33 s, 11.27 train_loss, 11.58 val_loss\n",
            "11100/30000 updates, 0.33 s, 10.58 train_loss, 11.65 val_loss\n",
            "11200/30000 updates, 0.35 s, 12.10 train_loss, 11.55 val_loss\n",
            "11300/30000 updates, 0.33 s, 11.63 train_loss, 11.57 val_loss\n",
            "11400/30000 updates, 0.39 s, 12.29 train_loss, 11.55 val_loss\n",
            "11500/30000 updates, 0.54 s, 11.82 train_loss, 11.58 val_loss\n",
            "11600/30000 updates, 0.51 s, 10.91 train_loss, 11.59 val_loss\n",
            "11700/30000 updates, 0.54 s, 12.16 train_loss, 11.63 val_loss\n",
            "11800/30000 updates, 0.51 s, 10.89 train_loss, 11.62 val_loss\n",
            "11900/30000 updates, 0.35 s, 10.23 train_loss, 11.55 val_loss\n",
            "12000/30000 updates, 0.34 s, 10.83 train_loss, 11.66 val_loss\n",
            "12100/30000 updates, 0.34 s, 10.48 train_loss, 11.61 val_loss\n",
            "12200/30000 updates, 0.35 s, 11.23 train_loss, 11.56 val_loss\n",
            "12300/30000 updates, 0.34 s, 10.92 train_loss, 11.58 val_loss\n",
            "12400/30000 updates, 0.34 s, 10.94 train_loss, 11.60 val_loss\n",
            "12500/30000 updates, 0.37 s, 11.74 train_loss, 11.61 val_loss\n",
            "12600/30000 updates, 0.34 s, 11.88 train_loss, 11.58 val_loss\n",
            "12700/30000 updates, 0.34 s, 12.30 train_loss, 11.62 val_loss\n",
            "12800/30000 updates, 0.35 s, 11.50 train_loss, 11.60 val_loss\n",
            "12900/30000 updates, 0.34 s, 10.07 train_loss, 11.59 val_loss\n",
            "13000/30000 updates, 0.34 s, 10.36 train_loss, 11.55 val_loss\n",
            "13100/30000 updates, 0.35 s, 11.60 train_loss, 11.54 val_loss\n",
            "13200/30000 updates, 0.34 s, 12.55 train_loss, 11.56 val_loss\n",
            "13300/30000 updates, 0.33 s, 11.76 train_loss, 11.52 val_loss\n",
            "13400/30000 updates, 0.35 s, 11.31 train_loss, 11.52 val_loss\n",
            "13500/30000 updates, 0.33 s, 11.27 train_loss, 11.52 val_loss\n",
            "13600/30000 updates, 0.34 s, 11.74 train_loss, 11.53 val_loss\n",
            "13700/30000 updates, 0.35 s, 11.28 train_loss, 11.62 val_loss\n",
            "13800/30000 updates, 0.34 s, 11.37 train_loss, 11.55 val_loss\n",
            "13900/30000 updates, 0.34 s, 10.94 train_loss, 11.54 val_loss\n",
            "14000/30000 updates, 0.35 s, 11.21 train_loss, 11.53 val_loss\n",
            "14100/30000 updates, 0.34 s, 11.10 train_loss, 11.53 val_loss\n",
            "14200/30000 updates, 0.35 s, 10.66 train_loss, 11.51 val_loss\n",
            "14300/30000 updates, 0.35 s, 12.03 train_loss, 11.52 val_loss\n",
            "14400/30000 updates, 0.37 s, 12.01 train_loss, 11.55 val_loss\n",
            "14500/30000 updates, 0.35 s, 10.50 train_loss, 11.52 val_loss\n",
            "14600/30000 updates, 0.38 s, 11.50 train_loss, 11.51 val_loss\n",
            "14700/30000 updates, 0.42 s, 11.30 train_loss, 11.54 val_loss\n",
            "14800/30000 updates, 0.52 s, 11.49 train_loss, 11.49 val_loss\n",
            "14900/30000 updates, 0.52 s, 11.12 train_loss, 11.52 val_loss\n",
            "15000/30000 updates, 0.53 s, 13.37 train_loss, 11.50 val_loss\n",
            "15100/30000 updates, 0.46 s, 11.05 train_loss, 11.58 val_loss\n",
            "15200/30000 updates, 0.34 s, 11.48 train_loss, 11.55 val_loss\n",
            "15300/30000 updates, 0.37 s, 10.56 train_loss, 11.50 val_loss\n",
            "15400/30000 updates, 0.34 s, 11.05 train_loss, 11.49 val_loss\n",
            "15500/30000 updates, 0.33 s, 11.25 train_loss, 11.48 val_loss\n",
            "15600/30000 updates, 0.35 s, 11.77 train_loss, 11.51 val_loss\n",
            "15700/30000 updates, 0.35 s, 12.55 train_loss, 11.50 val_loss\n",
            "15800/30000 updates, 0.32 s, 10.55 train_loss, 11.53 val_loss\n",
            "15900/30000 updates, 0.34 s, 11.34 train_loss, 11.50 val_loss\n",
            "16000/30000 updates, 0.33 s, 10.91 train_loss, 11.49 val_loss\n",
            "16100/30000 updates, 0.33 s, 10.67 train_loss, 11.47 val_loss\n",
            "16200/30000 updates, 0.35 s, 10.80 train_loss, 11.56 val_loss\n",
            "16300/30000 updates, 0.34 s, 10.88 train_loss, 11.56 val_loss\n",
            "16400/30000 updates, 0.33 s, 11.75 train_loss, 11.46 val_loss\n",
            "16500/30000 updates, 0.33 s, 11.83 train_loss, 11.52 val_loss\n",
            "16600/30000 updates, 0.34 s, 11.34 train_loss, 11.52 val_loss\n",
            "16700/30000 updates, 0.32 s, 11.28 train_loss, 11.63 val_loss\n",
            "16800/30000 updates, 0.33 s, 11.26 train_loss, 11.55 val_loss\n",
            "16900/30000 updates, 0.35 s, 11.92 train_loss, 11.60 val_loss\n",
            "17000/30000 updates, 0.34 s, 10.73 train_loss, 11.50 val_loss\n",
            "17100/30000 updates, 0.33 s, 11.30 train_loss, 11.49 val_loss\n",
            "17200/30000 updates, 0.35 s, 10.82 train_loss, 11.56 val_loss\n",
            "17300/30000 updates, 0.33 s, 9.68 train_loss, 11.46 val_loss\n",
            "17400/30000 updates, 0.34 s, 11.41 train_loss, 11.44 val_loss\n",
            "17500/30000 updates, 0.35 s, 12.60 train_loss, 11.45 val_loss\n",
            "17600/30000 updates, 0.33 s, 11.47 train_loss, 11.42 val_loss\n",
            "17700/30000 updates, 0.33 s, 11.84 train_loss, 11.52 val_loss\n",
            "17800/30000 updates, 0.35 s, 12.45 train_loss, 11.51 val_loss\n",
            "17900/30000 updates, 0.33 s, 10.93 train_loss, 11.48 val_loss\n",
            "18000/30000 updates, 0.33 s, 11.07 train_loss, 11.45 val_loss\n",
            "18100/30000 updates, 0.54 s, 10.62 train_loss, 11.45 val_loss\n",
            "18200/30000 updates, 0.52 s, 10.99 train_loss, 11.47 val_loss\n",
            "18300/30000 updates, 0.52 s, 11.71 train_loss, 11.49 val_loss\n",
            "18400/30000 updates, 0.56 s, 11.30 train_loss, 11.44 val_loss\n",
            "18500/30000 updates, 0.40 s, 11.44 train_loss, 11.45 val_loss\n",
            "18600/30000 updates, 0.33 s, 12.29 train_loss, 11.46 val_loss\n",
            "18700/30000 updates, 0.35 s, 11.14 train_loss, 11.44 val_loss\n",
            "18800/30000 updates, 0.34 s, 11.83 train_loss, 11.43 val_loss\n",
            "18900/30000 updates, 0.33 s, 11.19 train_loss, 11.53 val_loss\n",
            "19000/30000 updates, 0.34 s, 11.09 train_loss, 11.51 val_loss\n",
            "19100/30000 updates, 0.34 s, 13.08 train_loss, 11.48 val_loss\n",
            "19200/30000 updates, 0.33 s, 12.08 train_loss, 11.55 val_loss\n",
            "19300/30000 updates, 0.33 s, 11.96 train_loss, 11.47 val_loss\n",
            "19400/30000 updates, 0.34 s, 11.34 train_loss, 11.46 val_loss\n",
            "19500/30000 updates, 0.34 s, 10.74 train_loss, 11.43 val_loss\n",
            "19600/30000 updates, 0.34 s, 11.78 train_loss, 11.43 val_loss\n",
            "19700/30000 updates, 0.34 s, 11.52 train_loss, 11.45 val_loss\n",
            "19800/30000 updates, 0.32 s, 11.41 train_loss, 11.54 val_loss\n",
            "19900/30000 updates, 0.33 s, 11.78 train_loss, 11.42 val_loss\n",
            "20000/30000 updates, 0.34 s, 10.53 train_loss, 11.44 val_loss\n",
            "20100/30000 updates, 0.34 s, 10.98 train_loss, 11.51 val_loss\n",
            "20200/30000 updates, 0.34 s, 10.96 train_loss, 11.48 val_loss\n",
            "20300/30000 updates, 0.34 s, 10.93 train_loss, 11.48 val_loss\n",
            "20400/30000 updates, 0.34 s, 10.69 train_loss, 11.46 val_loss\n",
            "20500/30000 updates, 0.33 s, 10.95 train_loss, 11.42 val_loss\n",
            "20600/30000 updates, 0.34 s, 11.00 train_loss, 11.45 val_loss\n",
            "20700/30000 updates, 0.34 s, 11.71 train_loss, 11.46 val_loss\n",
            "20800/30000 updates, 0.33 s, 11.04 train_loss, 11.50 val_loss\n",
            "20900/30000 updates, 0.34 s, 10.85 train_loss, 11.51 val_loss\n",
            "21000/30000 updates, 0.33 s, 11.54 train_loss, 11.43 val_loss\n",
            "21100/30000 updates, 0.33 s, 10.83 train_loss, 11.43 val_loss\n",
            "21200/30000 updates, 0.36 s, 10.51 train_loss, 11.41 val_loss\n",
            "21300/30000 updates, 0.33 s, 11.25 train_loss, 11.45 val_loss\n",
            "21400/30000 updates, 0.38 s, 11.63 train_loss, 11.44 val_loss\n",
            "21500/30000 updates, 0.53 s, 11.01 train_loss, 11.45 val_loss\n",
            "21600/30000 updates, 0.50 s, 10.74 train_loss, 11.48 val_loss\n",
            "21700/30000 updates, 0.53 s, 11.81 train_loss, 11.42 val_loss\n",
            "21800/30000 updates, 0.54 s, 11.29 train_loss, 11.46 val_loss\n",
            "21900/30000 updates, 0.34 s, 11.66 train_loss, 11.44 val_loss\n",
            "22000/30000 updates, 0.34 s, 11.30 train_loss, 11.46 val_loss\n",
            "22100/30000 updates, 0.33 s, 11.87 train_loss, 11.45 val_loss\n",
            "22200/30000 updates, 0.34 s, 11.19 train_loss, 11.42 val_loss\n",
            "22300/30000 updates, 0.34 s, 10.98 train_loss, 11.52 val_loss\n",
            "22400/30000 updates, 0.33 s, 10.88 train_loss, 11.55 val_loss\n",
            "22500/30000 updates, 0.34 s, 12.55 train_loss, 11.41 val_loss\n",
            "22600/30000 updates, 0.34 s, 11.45 train_loss, 11.42 val_loss\n",
            "22700/30000 updates, 0.33 s, 11.46 train_loss, 11.42 val_loss\n",
            "22800/30000 updates, 0.34 s, 11.13 train_loss, 11.47 val_loss\n",
            "22900/30000 updates, 0.33 s, 10.88 train_loss, 11.45 val_loss\n",
            "23000/30000 updates, 0.33 s, 10.68 train_loss, 11.47 val_loss\n",
            "23100/30000 updates, 0.33 s, 21.86 train_loss, 11.41 val_loss\n",
            "23200/30000 updates, 0.35 s, 11.63 train_loss, 11.38 val_loss\n",
            "23300/30000 updates, 0.33 s, 10.84 train_loss, 11.45 val_loss\n",
            "23400/30000 updates, 0.33 s, 21.29 train_loss, 11.41 val_loss\n",
            "23500/30000 updates, 0.35 s, 11.62 train_loss, 11.41 val_loss\n",
            "23600/30000 updates, 0.33 s, 11.56 train_loss, 11.46 val_loss\n",
            "23700/30000 updates, 0.32 s, 10.40 train_loss, 11.42 val_loss\n",
            "23800/30000 updates, 0.35 s, 10.61 train_loss, 11.44 val_loss\n",
            "23900/30000 updates, 0.33 s, 10.96 train_loss, 11.43 val_loss\n",
            "24000/30000 updates, 0.34 s, 10.82 train_loss, 11.40 val_loss\n",
            "24100/30000 updates, 0.35 s, 11.90 train_loss, 11.41 val_loss\n",
            "24200/30000 updates, 0.33 s, 10.91 train_loss, 11.40 val_loss\n",
            "24300/30000 updates, 0.34 s, 11.98 train_loss, 11.38 val_loss\n",
            "24400/30000 updates, 0.35 s, 10.11 train_loss, 11.40 val_loss\n",
            "24500/30000 updates, 0.34 s, 11.09 train_loss, 11.50 val_loss\n",
            "24600/30000 updates, 0.33 s, 10.73 train_loss, 11.57 val_loss\n",
            "24700/30000 updates, 0.35 s, 11.56 train_loss, 11.44 val_loss\n",
            "24800/30000 updates, 0.44 s, 10.61 train_loss, 11.42 val_loss\n",
            "24900/30000 updates, 0.53 s, 11.08 train_loss, 11.43 val_loss\n",
            "25000/30000 updates, 0.50 s, 11.39 train_loss, 11.40 val_loss\n",
            "25100/30000 updates, 0.55 s, 12.28 train_loss, 11.39 val_loss\n",
            "25200/30000 updates, 0.44 s, 10.33 train_loss, 11.39 val_loss\n",
            "25300/30000 updates, 0.33 s, 10.93 train_loss, 11.43 val_loss\n",
            "25400/30000 updates, 0.35 s, 10.81 train_loss, 11.38 val_loss\n",
            "25500/30000 updates, 0.34 s, 11.12 train_loss, 11.37 val_loss\n",
            "25600/30000 updates, 0.33 s, 12.38 train_loss, 11.38 val_loss\n",
            "25700/30000 updates, 0.35 s, 11.94 train_loss, 11.42 val_loss\n",
            "25800/30000 updates, 0.33 s, 11.46 train_loss, 11.61 val_loss\n",
            "25900/30000 updates, 0.33 s, 10.59 train_loss, 11.43 val_loss\n",
            "26000/30000 updates, 0.35 s, 11.33 train_loss, 11.39 val_loss\n",
            "26100/30000 updates, 0.33 s, 12.38 train_loss, 11.45 val_loss\n",
            "26200/30000 updates, 0.33 s, 12.15 train_loss, 11.38 val_loss\n",
            "26300/30000 updates, 0.38 s, 11.86 train_loss, 11.42 val_loss\n",
            "26400/30000 updates, 0.33 s, 11.80 train_loss, 11.46 val_loss\n",
            "26500/30000 updates, 0.33 s, 11.26 train_loss, 11.44 val_loss\n",
            "26600/30000 updates, 0.35 s, 10.90 train_loss, 11.45 val_loss\n",
            "26700/30000 updates, 0.33 s, 11.53 train_loss, 11.39 val_loss\n",
            "26800/30000 updates, 0.36 s, 11.12 train_loss, 11.37 val_loss\n",
            "26900/30000 updates, 0.36 s, 12.02 train_loss, 11.43 val_loss\n",
            "27000/30000 updates, 0.33 s, 10.63 train_loss, 11.52 val_loss\n",
            "27100/30000 updates, 0.33 s, 11.15 train_loss, 11.46 val_loss\n",
            "27200/30000 updates, 0.35 s, 11.49 train_loss, 11.43 val_loss\n",
            "27300/30000 updates, 0.33 s, 11.11 train_loss, 11.42 val_loss\n",
            "27400/30000 updates, 0.33 s, 12.95 train_loss, 11.39 val_loss\n",
            "27500/30000 updates, 0.35 s, 11.58 train_loss, 11.37 val_loss\n",
            "27600/30000 updates, 0.33 s, 10.96 train_loss, 11.43 val_loss\n",
            "27700/30000 updates, 0.33 s, 11.54 train_loss, 11.40 val_loss\n",
            "27800/30000 updates, 0.36 s, 11.40 train_loss, 11.49 val_loss\n",
            "27900/30000 updates, 0.33 s, 11.72 train_loss, 11.44 val_loss\n",
            "28000/30000 updates, 0.34 s, 10.19 train_loss, 11.40 val_loss\n",
            "28100/30000 updates, 0.38 s, 10.81 train_loss, 11.42 val_loss\n",
            "28200/30000 updates, 0.53 s, 10.86 train_loss, 11.44 val_loss\n",
            "28300/30000 updates, 0.53 s, 12.40 train_loss, 11.42 val_loss\n",
            "28400/30000 updates, 0.54 s, 11.21 train_loss, 11.44 val_loss\n",
            "28500/30000 updates, 0.54 s, 11.93 train_loss, 11.40 val_loss\n",
            "28600/30000 updates, 0.34 s, 11.60 train_loss, 11.40 val_loss\n",
            "28700/30000 updates, 0.35 s, 10.33 train_loss, 11.37 val_loss\n",
            "28800/30000 updates, 0.35 s, 11.92 train_loss, 11.43 val_loss\n",
            "28900/30000 updates, 0.34 s, 11.18 train_loss, 11.60 val_loss\n",
            "29000/30000 updates, 0.33 s, 11.01 train_loss, 11.46 val_loss\n",
            "29100/30000 updates, 0.35 s, 10.98 train_loss, 11.76 val_loss\n",
            "29200/30000 updates, 0.33 s, 11.50 train_loss, 11.42 val_loss\n",
            "29300/30000 updates, 0.34 s, 10.35 train_loss, 11.38 val_loss\n",
            "29400/30000 updates, 0.35 s, 11.84 train_loss, 11.39 val_loss\n",
            "29500/30000 updates, 0.34 s, 11.09 train_loss, 11.39 val_loss\n",
            "29600/30000 updates, 0.34 s, 11.36 train_loss, 11.37 val_loss\n",
            "29700/30000 updates, 0.35 s, 11.02 train_loss, 11.40 val_loss\n",
            "29800/30000 updates, 0.33 s, 11.40 train_loss, 11.38 val_loss\n",
            "29900/30000 updates, 0.33 s, 10.38 train_loss, 11.36 val_loss\n"
          ]
        }
      ],
      "source": [
        "batch_pointer = 0\n",
        "\n",
        "start = time.time()\n",
        "best = float(\"inf\")\n",
        "\n",
        "\n",
        "for i in range(max_iter):\n",
        "    x_batch = Xz[batch_pointer: batch_pointer + batch_size, :]\n",
        "    s_batch = S[batch_pointer: batch_pointer + batch_size, :]\n",
        "\n",
        "    _, _loss, _step = sess.run([train_op, loss, global_step], {x_pl: x_batch, s_pl: s_batch, n_pl: n_samples})\n",
        "\n",
        "    batch_pointer += batch_size\n",
        "\n",
        "    if batch_pointer > N - batch_size:\n",
        "        batch_pointer = 0\n",
        "\n",
        "        p = np.random.permutation(N)\n",
        "        Xz = Xz[p, :]\n",
        "        S = S[p, :]\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        took = time.time() - start\n",
        "        start = time.time()\n",
        "\n",
        "        # --- change the following batch if you want a true validation set\n",
        "        x_batch = Xz\n",
        "        s_batch = S\n",
        "\n",
        "        val_loss, _step = sess.run([loss, global_step], {x_pl: x_batch, s_pl: s_batch, n_pl: n_samples})\n",
        "\n",
        "        print(\"{0}/{1} updates, {2:.2f} s, {3:.2f} train_loss, {4:.2f} val_loss\".format(i, max_iter, took, _loss, val_loss))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTDeqc0ts1LB"
      },
      "source": [
        "### Single imputation RMSE\n",
        "The *self-normalized importance sampling* approach for the MIWAE is described in this [paper](https://arxiv.org/pdf/1812.02633.pdf). This needs to be modified slightly in the MNAR case to account for the missing model, as described in the not-MIWAE paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "collapsed": true,
        "id": "vuPuEwkgs1LC"
      },
      "outputs": [],
      "source": [
        "def imputationRMSE(sess, Xorg, Xnan, L):\n",
        "\n",
        "    N = len(Xorg)\n",
        "\n",
        "    Xz = Xnan.copy()\n",
        "    Xz[np.isnan(Xnan)] = 0\n",
        "    S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
        "\n",
        "    def softmax(x):\n",
        "        e_x = np.exp(x - np.max(x, axis=1)[:, None])\n",
        "        return e_x / e_x.sum(axis=1)[:, None]\n",
        "\n",
        "    def imp(xz, s, L):\n",
        "        _mu, _log_p_x_given_z, _log_p_z, _log_q_z_given_x = sess.run(\n",
        "            [mu, log_p_x_given_z, log_p_z, log_q_z_given_x],\n",
        "            {x_pl: xz, s_pl: s, n_pl: L})\n",
        "\n",
        "        wl = softmax(_log_p_x_given_z + _log_p_z - _log_q_z_given_x)\n",
        "\n",
        "        xm = np.sum((_mu.T * wl.T).T, axis=1)\n",
        "        xmix = xz + xm * (1 - s)\n",
        "\n",
        "        return _mu, wl, xm, xmix\n",
        "\n",
        "    XM = np.zeros_like(Xorg)\n",
        "\n",
        "    for i in range(N):\n",
        "\n",
        "        xz = Xz[i, :][None, :]\n",
        "        s = S[i, :][None, :]\n",
        "\n",
        "        _mu, wl, xm, xmix = imp(xz, s, L)\n",
        "\n",
        "        XM[i, :] = xm\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('{0} / {1}'.format(i, N))\n",
        "\n",
        "    return np.sqrt(np.sum((Xorg - XM) ** 2 * (1 - S)) / np.sum(1 - S)), XM\n",
        "\n",
        "\n",
        "def not_imputationRMSE(sess, Xorg, Xnan, L):\n",
        "\n",
        "    N = len(Xorg)\n",
        "\n",
        "    Xz = Xnan.copy()\n",
        "    Xz[np.isnan(Xnan)] = 0\n",
        "    S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
        "\n",
        "    def softmax(x):\n",
        "        e_x = np.exp(x - np.max(x, axis=1)[:, None])\n",
        "        return e_x / e_x.sum(axis=1)[:, None]\n",
        "\n",
        "    def imp(xz, s, L):\n",
        "        _mu, _log_p_x_given_z, _log_p_z, _log_q_z_given_x, _log_p_s_given_x  = sess.run(\n",
        "            [mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x],\n",
        "            {x_pl: xz, s_pl: s, n_pl: L})\n",
        "\n",
        "        wl = softmax(_log_p_x_given_z + _log_p_s_given_x + _log_p_z - _log_q_z_given_x)\n",
        "\n",
        "        xm = np.sum((_mu.T * wl.T).T, axis=1)\n",
        "        xmix = xz + xm * (1 - s)\n",
        "\n",
        "        return _mu, wl, xm, xmix\n",
        "\n",
        "    XM = np.zeros_like(Xorg)\n",
        "\n",
        "    for i in range(N):\n",
        "\n",
        "        xz = Xz[i, :][None, :]\n",
        "        s = S[i, :][None, :]\n",
        "\n",
        "        _mu, wl, xm, xmix = imp(xz, s, L)\n",
        "\n",
        "        XM[i, :] = xm\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('{0} / {1}'.format(i, N))\n",
        "\n",
        "    return np.sqrt(np.sum((Xorg - XM) ** 2 * (1 - S)) / np.sum(1 - S)), XM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI6X9BpGs1LC"
      },
      "source": [
        "### Calculate the single imputation RMSE using 10k importance samples\n",
        "If you used the MIWAE loss use the imputationRMSE\n",
        "\n",
        "If you used the notMIWAE loss use the not_imputationRMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mipqD-WJs1LC",
        "outputId": "32afc383-164a-4ce4-f1ea-2be8388635e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 4898\n",
            "100 / 4898\n",
            "200 / 4898\n",
            "300 / 4898\n",
            "400 / 4898\n",
            "500 / 4898\n",
            "600 / 4898\n",
            "700 / 4898\n",
            "800 / 4898\n",
            "900 / 4898\n",
            "1000 / 4898\n",
            "1100 / 4898\n",
            "1200 / 4898\n",
            "1300 / 4898\n",
            "1400 / 4898\n",
            "1500 / 4898\n",
            "1600 / 4898\n",
            "1700 / 4898\n",
            "1800 / 4898\n",
            "1900 / 4898\n",
            "2000 / 4898\n",
            "2100 / 4898\n",
            "2200 / 4898\n",
            "2300 / 4898\n",
            "2400 / 4898\n",
            "2500 / 4898\n",
            "2600 / 4898\n",
            "2700 / 4898\n",
            "2800 / 4898\n",
            "2900 / 4898\n",
            "3000 / 4898\n",
            "3100 / 4898\n",
            "3200 / 4898\n",
            "3300 / 4898\n",
            "3400 / 4898\n",
            "3500 / 4898\n",
            "3600 / 4898\n",
            "3700 / 4898\n",
            "3800 / 4898\n",
            "3900 / 4898\n",
            "4000 / 4898\n",
            "4100 / 4898\n",
            "4200 / 4898\n",
            "4300 / 4898\n",
            "4400 / 4898\n",
            "4500 / 4898\n",
            "4600 / 4898\n",
            "4700 / 4898\n",
            "4800 / 4898\n",
            "imputation RMSE:  1.0335387005885028\n"
          ]
        }
      ],
      "source": [
        "# ---- S has been permuted during training, so just reinstantiate it\n",
        "S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
        "\n",
        "rmse, imputations = not_imputationRMSE(sess, Xtrain, Xnan, 10000)\n",
        "# rmse, imputations = imputationRMSE(sess, Xtrain, Xnan, 10000)\n",
        "\n",
        "print(\"imputation RMSE: \", rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWA6-7dCs1LD"
      },
      "source": [
        "### Compare to missForest and MICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "collapsed": true,
        "id": "eilHlvTis1LD"
      },
      "outputs": [],
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "WffqDLCGs1LD",
        "outputId": "d40fea9b-57c0-4119-ec17-e43670c16a40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "estimator = RandomForestRegressor(n_estimators=100)\n",
        "imp = IterativeImputer(estimator=estimator)\n",
        "imp.fit(Xnan)\n",
        "Xrec = imp.transform(Xnan)\n",
        "rmse_mf = np.sqrt(np.sum((Xtrain - Xrec) ** 2 * (1 - S)) / np.sum(1 - S))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "4kxDnjMTs1LD",
        "outputId": "8d746581-e614-4d06-a30e-4f630a893f23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missForst imputation RMSE:  1.6216428516396266\n"
          ]
        }
      ],
      "source": [
        "print(\"missForst imputation RMSE: \", rmse_mf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "collapsed": true,
        "id": "KA7twgn-s1LD"
      },
      "outputs": [],
      "source": [
        "imp = IterativeImputer(max_iter=100)\n",
        "imp.fit(Xnan)\n",
        "Xrec = imp.transform(Xnan)\n",
        "RMSE_iter = np.sqrt(np.sum((Xtrain - Xrec) ** 2 * (1 - S)) / np.sum(1 - S))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Te1ajfCbs1LE",
        "outputId": "606cb2d5-3a0a-49d6-afdf-b7902f6f3a72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MICE, imputation RMSE 1.4102763723316911\n"
          ]
        }
      ],
      "source": [
        "print(\"MICE, imputation RMSE\", RMSE_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfqoem1qs1LE"
      },
      "source": [
        "### Inspect the learned missing model\n",
        "There is a separate missing process in each feature dimesion, inspect each of them, plot as function of feature value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "0G3ANjWts1LE",
        "outputId": "5618930c-8c65-4d85-dfb7-bf8b46dad789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "_W, _b = sess.run([W, b])\n",
        "x = np.linspace(-2, 2, 100)\n",
        "\n",
        "def sigmoid(logits):\n",
        "    return 1 / (1 + np.exp(-logits))\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "for d in range(D // 2):\n",
        "    logits = _W[0][0][d] * (x - _b[0][0][d])\n",
        "    psx = sigmoid(logits)\n",
        "    plt.plot(x, psx)\n",
        "\n",
        "plt.xlabel('feature value')\n",
        "plt.ylabel('$p(s|x)$')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUtckziTs1LE"
      },
      "source": [
        "This should illustrate that the probability of the feature value being observed when it is below the feature mean should be close to 1, while the probability of being observed above the feature mean should be close to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKulK-Y9s1LF"
      },
      "source": [
        "### Close the session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "collapsed": true,
        "id": "0oOGwEAzs1LF"
      },
      "outputs": [],
      "source": [
        "# sess.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}