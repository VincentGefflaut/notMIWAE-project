{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIWAE and not-MIWAE on SVHN Dataset\n",
    "\n",
    "This notebook implements the clipping experiment from the not-MIWAE paper (Section 4.3).\n",
    "We emulate a clipping phenomenon where pixel values above a threshold are set to missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 20:41:17.808145: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/qy/p2z3khpd46vd6qf003_n4f480000gn/T/ipykernel_55487/3743708471.py:14: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU is NOT available, using CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Check GPU availability\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(f\"GPU Device: {tf.test.gpu_device_name()}\")\n",
    "else:\n",
    "    print(\"GPU is NOT available, using CPU.\")\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams['font.size'] = 12.0\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load SVHN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SVHN dataset if not present\n",
    "import urllib.request\n",
    "\n",
    "data_dir = 'data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_url = 'http://ufldl.stanford.edu/housenumbers/train_32x32.mat'\n",
    "test_url = 'http://ufldl.stanford.edu/housenumbers/test_32x32.mat'\n",
    "\n",
    "train_path = os.path.join(data_dir, 'train_32x32.mat')\n",
    "test_path = os.path.join(data_dir, 'test_32x32.mat')\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    print(\"Downloading SVHN training data...\")\n",
    "    urllib.request.urlretrieve(train_url, train_path)\n",
    "    print(\"Done!\")\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    print(\"Downloading SVHN test data...\")\n",
    "    urllib.request.urlretrieve(test_url, test_path)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "train_data = loadmat(train_path)\n",
    "test_data = loadmat(test_path)\n",
    "\n",
    "# Extract images and convert to grayscale, normalize to [0, 1]\n",
    "def preprocess_svhn(data):\n",
    "    X = data['X'].astype(np.float32)\n",
    "    # Transpose from (32, 32, 3, N) to (N, 32, 32, 3)\n",
    "    X = np.transpose(X, (3, 0, 1, 2))\n",
    "    # Convert to grayscale\n",
    "    X = np.mean(X, axis=-1, keepdims=True)\n",
    "    # Normalize to [0, 1]\n",
    "    X = X / 255.0\n",
    "    return X\n",
    "\n",
    "X_train = preprocess_svhn(train_data)\n",
    "X_test = preprocess_svhn(test_data)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Use a subset for faster training (optional)\n",
    "N_train = 10000  # Use 10k samples for training\n",
    "N_test = 2000    # Use 2k samples for testing\n",
    "\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.choice(len(X_train), N_train, replace=False)\n",
    "test_idx = np.random.choice(len(X_test), N_test, replace=False)\n",
    "\n",
    "X_train = X_train[train_idx]\n",
    "X_test = X_test[test_idx]\n",
    "\n",
    "print(f\"Using {N_train} training samples and {N_test} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introduce Missing Data (Clipping)\n",
    "\n",
    "We use a self-masking mechanism identical to the paper:\n",
    "$$P(s_{ij} = 1 | x_{ij}) = \\frac{1}{1 + e^{-\\text{logits}}}, \\quad \\text{logits} = W(x_{ij} - b)$$\n",
    "\n",
    "where $W = -50$ and $b = 0.75$ (clipping point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing process parameters (from paper)\n",
    "W_miss = -50.0\n",
    "b_miss = 0.75\n",
    "\n",
    "def introduce_missing(X, W, b):\n",
    "    \"\"\"Introduce MNAR missing data via clipping mechanism.\"\"\"\n",
    "    logits = W * (X - b)\n",
    "    prob_observed = 1.0 / (1.0 + np.exp(-logits))\n",
    "    S = np.random.binomial(1, prob_observed).astype(np.float32)\n",
    "    \n",
    "    # Create masked version (0 where missing)\n",
    "    X_masked = X * S\n",
    "    \n",
    "    return X_masked, S\n",
    "\n",
    "# Apply missing mechanism to training and test data\n",
    "X_train_masked, S_train = introduce_missing(X_train, W_miss, b_miss)\n",
    "X_test_masked, S_test = introduce_missing(X_test, W_miss, b_miss)\n",
    "\n",
    "# Calculate missing rate\n",
    "missing_rate_train = 1 - np.mean(S_train)\n",
    "missing_rate_test = 1 - np.mean(S_test)\n",
    "print(f\"Training missing rate: {missing_rate_train:.2%}\")\n",
    "print(f\"Test missing rate: {missing_rate_test:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(X_train[i, :, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0, i].set_title('Original')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(X_train_masked[i, :, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1, i].set_title('With Missing')\n",
    "    axes[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "n_latent = 20\n",
    "n_samples = 5  # Number of importance samples (K)\n",
    "batch_size = 64\n",
    "max_iter = 50000  # Reduce for faster experimentation\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Data dimensions\n",
    "img_height, img_width, img_channels = 32, 32, 1\n",
    "D = img_height * img_width * img_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build the Model\n",
    "\n",
    "We build a convolutional VAE with:\n",
    "- Encoder: Conv layers -> latent distribution\n",
    "- Decoder: Dense + ConvTranspose layers -> reconstructed image\n",
    "- Missing model: Logistic regression per pixel (for not-MIWAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building computation graph...\")\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "with tf.variable_scope('input'):\n",
    "    x_pl = tf.placeholder(tf.float32, [None, img_height, img_width, img_channels], 'x_pl')\n",
    "    s_pl = tf.placeholder(tf.float32, [None, img_height, img_width, img_channels], 's_pl')\n",
    "    n_pl = tf.placeholder(tf.int32, shape=(), name='n_pl')\n",
    "    is_training = tf.placeholder(tf.bool, shape=(), name='is_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder (Convolutional)\n",
    "with tf.variable_scope('encoder'):\n",
    "    # Input: [batch, 32, 32, 1]\n",
    "    enc = keras.layers.Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu)(x_pl)\n",
    "    # [batch, 16, 16, 64]\n",
    "    enc = keras.layers.Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu)(enc)\n",
    "    # [batch, 8, 8, 128]\n",
    "    enc = keras.layers.Conv2D(256, 4, strides=2, padding='same', activation=tf.nn.relu)(enc)\n",
    "    # [batch, 4, 4, 256]\n",
    "    enc = keras.layers.Flatten()(enc)\n",
    "    # [batch, 4096]\n",
    "    \n",
    "    q_mu = keras.layers.Dense(n_latent, activation=None, name='q_mu')(enc)\n",
    "    q_logstd = keras.layers.Dense(n_latent, activation=lambda x: tf.clip_by_value(x, -10, 10), name='q_logstd')(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational distribution and sampling\n",
    "with tf.variable_scope('variational'):\n",
    "    q_z = tfp.distributions.Normal(loc=q_mu, scale=tf.exp(q_logstd))\n",
    "    \n",
    "    # Sample latent variables: [n_samples, batch, n_latent] -> [batch, n_samples, n_latent]\n",
    "    l_z = q_z.sample(n_pl)\n",
    "    l_z = tf.transpose(l_z, perm=[1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder (Convolutional Transpose)\n",
    "with tf.variable_scope('decoder'):\n",
    "    # Reshape for convolutional layers\n",
    "    batch_size_dynamic = tf.shape(l_z)[0]\n",
    "    n_samples_dynamic = tf.shape(l_z)[1]\n",
    "    \n",
    "    # Flatten batch and samples: [batch * n_samples, n_latent]\n",
    "    l_z_flat = tf.reshape(l_z, [-1, n_latent])\n",
    "    \n",
    "    dec = keras.layers.Dense(4 * 4 * 256, activation=tf.nn.relu)(l_z_flat)\n",
    "    dec = tf.reshape(dec, [-1, 4, 4, 256])\n",
    "    \n",
    "    dec = keras.layers.Conv2DTranspose(256, 4, strides=2, padding='same', activation=tf.nn.relu)(dec)\n",
    "    # [batch * n_samples, 8, 8, 256]\n",
    "    dec = keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same', activation=tf.nn.relu)(dec)\n",
    "    # [batch * n_samples, 16, 16, 128]\n",
    "    dec = keras.layers.Conv2DTranspose(64, 4, strides=2, padding='same', activation=tf.nn.relu)(dec)\n",
    "    # [batch * n_samples, 32, 32, 64]\n",
    "    \n",
    "    # Output mean (with sigmoid to bound [0, 1])\n",
    "    mu_flat = keras.layers.Conv2DTranspose(1, 4, strides=1, padding='same', activation=tf.nn.sigmoid)(dec)\n",
    "    # [batch * n_samples, 32, 32, 1]\n",
    "    \n",
    "    # Reshape back: [batch, n_samples, 32, 32, 1]\n",
    "    mu = tf.reshape(mu_flat, [batch_size_dynamic, n_samples_dynamic, img_height, img_width, img_channels])\n",
    "\n",
    "# Learned observation noise (lower bounded)\n",
    "with tf.variable_scope('data_process'):\n",
    "    logstd = tf.get_variable('logstd', shape=[], initializer=tf.constant_initializer(-2.0))\n",
    "    obs_std = tf.maximum(tf.exp(logstd), 0.02)  # Lower bound ~0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation model p(x|z)\n",
    "with tf.variable_scope('observation'):\n",
    "    p_x_given_z = tfp.distributions.Normal(loc=mu, scale=obs_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing model p(s|x) for not-MIWAE\n",
    "# We use the true missing mechanism (known)\n",
    "with tf.variable_scope('missing_model'):\n",
    "    # Mix observed data with samples from decoder for missing values\n",
    "    # x_pl: [batch, 32, 32, 1], s_pl: [batch, 32, 32, 1]\n",
    "    # mu: [batch, n_samples, 32, 32, 1]\n",
    "    \n",
    "    x_expanded = tf.expand_dims(x_pl, axis=1)  # [batch, 1, 32, 32, 1]\n",
    "    s_expanded = tf.expand_dims(s_pl, axis=1)  # [batch, 1, 32, 32, 1]\n",
    "    \n",
    "    # Mix: use observed where available, sampled where missing\n",
    "    l_out_mixed = mu * (1 - s_expanded) + x_expanded * s_expanded\n",
    "    \n",
    "    # True missing model parameters (from paper)\n",
    "    W_missing = tf.constant(-50.0, dtype=tf.float32)\n",
    "    b_missing = tf.constant(0.75, dtype=tf.float32)\n",
    "    \n",
    "    logits_missing = W_missing * (l_out_mixed - b_missing)\n",
    "    p_s_given_x = tfp.distributions.Bernoulli(logits=logits_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate log-likelihoods\n",
    "with tf.variable_scope('log_likelihoods'):\n",
    "    # Log p(x^o | z) - only evaluate on observed pixels\n",
    "    log_p_x_given_z = tf.reduce_sum(\n",
    "        s_expanded * p_x_given_z.log_prob(x_expanded),\n",
    "        axis=[2, 3, 4]  # Sum over spatial dimensions\n",
    "    )  # [batch, n_samples]\n",
    "    \n",
    "    # Log q(z | x^o)\n",
    "    q_z2 = tfp.distributions.Normal(\n",
    "        loc=tf.expand_dims(q_z.loc, axis=1),\n",
    "        scale=tf.expand_dims(q_z.scale, axis=1)\n",
    "    )\n",
    "    log_q_z_given_x = tf.reduce_sum(q_z2.log_prob(l_z), axis=-1)  # [batch, n_samples]\n",
    "    \n",
    "    # Log p(z) - standard normal prior\n",
    "    prior = tfp.distributions.Normal(loc=0.0, scale=1.0)\n",
    "    log_p_z = tf.reduce_sum(prior.log_prob(l_z), axis=-1)  # [batch, n_samples]\n",
    "    \n",
    "    # Log p(s | x) for not-MIWAE\n",
    "    log_p_s_given_x = tf.reduce_sum(\n",
    "        p_s_given_x.log_prob(s_expanded),\n",
    "        axis=[2, 3, 4]  # Sum over spatial dimensions\n",
    "    )  # [batch, n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIWAE and not-MIWAE objectives\n",
    "with tf.variable_scope('objectives'):\n",
    "    # MIWAE: ignores missing mechanism\n",
    "    l_w_miwae = log_p_x_given_z + log_p_z - log_q_z_given_x\n",
    "    log_sum_w_miwae = tf.reduce_logsumexp(l_w_miwae, axis=1)\n",
    "    log_avg_w_miwae = log_sum_w_miwae - tf.log(tf.cast(n_pl, tf.float32))\n",
    "    MIWAE = tf.reduce_mean(log_avg_w_miwae)\n",
    "    \n",
    "    # not-MIWAE: includes missing mechanism\n",
    "    l_w_notmiwae = log_p_x_given_z + log_p_s_given_x + log_p_z - log_q_z_given_x\n",
    "    log_sum_w_notmiwae = tf.reduce_logsumexp(l_w_notmiwae, axis=1)\n",
    "    log_avg_w_notmiwae = log_sum_w_notmiwae - tf.log(tf.cast(n_pl, tf.float32))\n",
    "    notMIWAE = tf.reduce_mean(log_avg_w_notmiwae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training operations\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "global_step = tf.Variable(initial_value=0, trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Get trainable variables\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "# Separate training ops for MIWAE and not-MIWAE\n",
    "loss_miwae = -MIWAE\n",
    "loss_notmiwae = -notMIWAE\n",
    "\n",
    "train_op_miwae = optimizer.minimize(loss_miwae, global_step=global_step, var_list=tvars)\n",
    "\n",
    "# Reset optimizer for not-MIWAE\n",
    "optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "global_step2 = tf.Variable(initial_value=0, trainable=False)\n",
    "train_op_notmiwae = optimizer2.minimize(loss_notmiwae, global_step=global_step2, var_list=tvars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(sess, train_op, loss_op, model_name, X_data, S_data, max_iter, batch_size, n_samples):\n",
    "    \"\"\"Train either MIWAE or not-MIWAE.\"\"\"\n",
    "    N = len(X_data)\n",
    "    batch_pointer = 0\n",
    "    \n",
    "    # Shuffle data\n",
    "    perm = np.random.permutation(N)\n",
    "    X_shuffled = X_data[perm]\n",
    "    S_shuffled = S_data[perm]\n",
    "    \n",
    "    losses = []\n",
    "    start = time.time()\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Get batch\n",
    "        x_batch = X_shuffled[batch_pointer:batch_pointer + batch_size]\n",
    "        s_batch = S_shuffled[batch_pointer:batch_pointer + batch_size]\n",
    "        \n",
    "        # Train step\n",
    "        _, _loss = sess.run(\n",
    "            [train_op, loss_op],\n",
    "            {x_pl: x_batch, s_pl: s_batch, n_pl: n_samples, is_training: True}\n",
    "        )\n",
    "        \n",
    "        losses.append(_loss)\n",
    "        batch_pointer += batch_size\n",
    "        \n",
    "        # Reset batch pointer and reshuffle\n",
    "        if batch_pointer > N - batch_size:\n",
    "            batch_pointer = 0\n",
    "            perm = np.random.permutation(N)\n",
    "            X_shuffled = X_data[perm]\n",
    "            S_shuffled = S_data[perm]\n",
    "        \n",
    "        # Print progress\n",
    "        if i % 1000 == 0:\n",
    "            elapsed = time.time() - start\n",
    "            avg_loss = np.mean(losses[-1000:]) if len(losses) >= 1000 else np.mean(losses)\n",
    "            print(f\"{model_name} - Iter {i}/{max_iter}, Loss: {avg_loss:.2f}, Time: {elapsed:.1f}s\")\n",
    "            start = time.time()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Imputation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_miwae(sess, X_masked, S, L=1000):\n",
    "    \"\"\"Impute missing values using MIWAE (ignoring missing mechanism).\"\"\"\n",
    "    N = len(X_masked)\n",
    "    imputations = np.zeros_like(X_masked)\n",
    "    \n",
    "    for i in range(N):\n",
    "        x = X_masked[i:i+1]\n",
    "        s = S[i:i+1]\n",
    "        \n",
    "        # Get samples and weights\n",
    "        _mu, _log_p_x_given_z, _log_p_z, _log_q_z_given_x = sess.run(\n",
    "            [mu, log_p_x_given_z, log_p_z, log_q_z_given_x],\n",
    "            {x_pl: x, s_pl: s, n_pl: L, is_training: False}\n",
    "        )\n",
    "        \n",
    "        # Compute importance weights (softmax)\n",
    "        log_w = _log_p_x_given_z + _log_p_z - _log_q_z_given_x\n",
    "        log_w = log_w - np.max(log_w)  # Numerical stability\n",
    "        w = np.exp(log_w)\n",
    "        w = w / np.sum(w)\n",
    "        \n",
    "        # Weighted average of reconstructions\n",
    "        # _mu: [1, L, 32, 32, 1]\n",
    "        imp = np.sum(_mu[0] * w[0, :, None, None, None], axis=0)\n",
    "        \n",
    "        # Mix with observed values\n",
    "        imputations[i] = x[0] * s[0] + imp * (1 - s[0])\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f\"MIWAE imputation: {i}/{N}\")\n",
    "    \n",
    "    return imputations\n",
    "\n",
    "\n",
    "def impute_notmiwae(sess, X_masked, S, L=1000):\n",
    "    \"\"\"Impute missing values using not-MIWAE (including missing mechanism).\"\"\"\n",
    "    N = len(X_masked)\n",
    "    imputations = np.zeros_like(X_masked)\n",
    "    \n",
    "    for i in range(N):\n",
    "        x = X_masked[i:i+1]\n",
    "        s = S[i:i+1]\n",
    "        \n",
    "        # Get samples and weights\n",
    "        _mu, _log_p_x_given_z, _log_p_z, _log_q_z_given_x, _log_p_s_given_x = sess.run(\n",
    "            [mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x],\n",
    "            {x_pl: x, s_pl: s, n_pl: L, is_training: False}\n",
    "        )\n",
    "        \n",
    "        # Compute importance weights including missing model\n",
    "        log_w = _log_p_x_given_z + _log_p_s_given_x + _log_p_z - _log_q_z_given_x\n",
    "        log_w = log_w - np.max(log_w)\n",
    "        w = np.exp(log_w)\n",
    "        w = w / np.sum(w)\n",
    "        \n",
    "        # Weighted average\n",
    "        imp = np.sum(_mu[0] * w[0, :, None, None, None], axis=0)\n",
    "        \n",
    "        # Mix with observed\n",
    "        imputations[i] = x[0] * s[0] + imp * (1 - s[0])\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f\"not-MIWAE imputation: {i}/{N}\")\n",
    "    \n",
    "    return imputations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train MIWAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session and train MIWAE\n",
    "sess_miwae = tf.Session(config=config)\n",
    "sess_miwae.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"Training MIWAE...\")\n",
    "losses_miwae = train_model(\n",
    "    sess_miwae, train_op_miwae, loss_miwae, \"MIWAE\",\n",
    "    X_train_masked, S_train, max_iter, batch_size, n_samples\n",
    ")\n",
    "print(\"MIWAE training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with MIWAE\n",
    "print(\"\\nComputing MIWAE imputations...\")\n",
    "imputations_miwae = impute_miwae(sess_miwae, X_test_masked, S_test, L=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MIWAE RMSE\n",
    "rmse_miwae = np.sqrt(\n",
    "    np.sum((X_test - imputations_miwae) ** 2 * (1 - S_test)) / np.sum(1 - S_test)\n",
    ")\n",
    "print(f\"MIWAE Imputation RMSE: {rmse_miwae:.5f}\")\n",
    "\n",
    "# Constant imputation baseline (impute with 1's since values > 0.75 are clipped)\n",
    "constant_imp = X_test_masked + (1 - S_test) * 1.0\n",
    "rmse_constant = np.sqrt(\n",
    "    np.sum((X_test - constant_imp) ** 2 * (1 - S_test)) / np.sum(1 - S_test)\n",
    ")\n",
    "print(f\"Constant (1's) Imputation RMSE: {rmse_constant:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train not-MIWAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new session for not-MIWAE\n",
    "sess_notmiwae = tf.Session(config=config)\n",
    "sess_notmiwae.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"Training not-MIWAE...\")\n",
    "losses_notmiwae = train_model(\n",
    "    sess_notmiwae, train_op_notmiwae, loss_notmiwae, \"not-MIWAE\",\n",
    "    X_train_masked, S_train, max_iter, batch_size, n_samples\n",
    ")\n",
    "print(\"not-MIWAE training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with not-MIWAE\n",
    "print(\"\\nComputing not-MIWAE imputations...\")\n",
    "imputations_notmiwae = impute_notmiwae(sess_notmiwae, X_test_masked, S_test, L=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate not-MIWAE RMSE\n",
    "rmse_notmiwae = np.sqrt(\n",
    "    np.sum((X_test - imputations_notmiwae) ** 2 * (1 - S_test)) / np.sum(1 - S_test)\n",
    ")\n",
    "print(f\"not-MIWAE Imputation RMSE: {rmse_notmiwae:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Constant Imputation RMSE: {rmse_constant:.5f}\")\n",
    "print(f\"MIWAE Imputation RMSE:    {rmse_miwae:.5f}\")\n",
    "print(f\"not-MIWAE Imputation RMSE: {rmse_notmiwae:.5f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Smooth losses for visualization\n",
    "window = 100\n",
    "losses_miwae_smooth = np.convolve(losses_miwae, np.ones(window)/window, mode='valid')\n",
    "losses_notmiwae_smooth = np.convolve(losses_notmiwae, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax1.plot(losses_miwae_smooth, label='MIWAE')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('MIWAE Training Loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(losses_notmiwae_smooth, label='not-MIWAE', color='orange')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('not-MIWAE Training Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize imputations\n",
    "n_examples = 5\n",
    "fig, axes = plt.subplots(4, n_examples, figsize=(12, 10))\n",
    "\n",
    "for i in range(n_examples):\n",
    "    idx = i * 50  # Spread out examples\n",
    "    \n",
    "    # Original\n",
    "    axes[0, i].imshow(X_test[idx, :, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0, i].set_title('Original' if i == 0 else '')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # With missing\n",
    "    axes[1, i].imshow(X_test_masked[idx, :, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1, i].set_title('Missing' if i == 0 else '')\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # MIWAE imputation\n",
    "    axes[2, i].imshow(imputations_miwae[idx, :, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[2, i].set_title('MIWAE' if i == 0 else '')\n",
    "    axes[2, i].axis('off')\n",
    "    \n",
    "    # not-MIWAE imputation\n",
    "    axes[3, i].imshow(imputations_notmiwae[idx, :, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[3, i].set_title('not-MIWAE' if i == 0 else '')\n",
    "    axes[3, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('svhn_imputations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of imputed values (as in paper Figure 2)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Get only the imputed (missing) values\n",
    "missing_mask = (1 - S_test).astype(bool)\n",
    "\n",
    "miwae_imputed_vals = imputations_miwae[missing_mask]\n",
    "notmiwae_imputed_vals = imputations_notmiwae[missing_mask]\n",
    "true_missing_vals = X_test[missing_mask]\n",
    "\n",
    "axes[0].hist(miwae_imputed_vals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Imputation value')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('(a) MIWAE')\n",
    "axes[0].set_xlim([0, 1])\n",
    "\n",
    "axes[1].hist(notmiwae_imputed_vals, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Imputation value')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('(b) not-MIWAE')\n",
    "axes[1].set_xlim([0, 1])\n",
    "\n",
    "axes[2].hist(true_missing_vals, bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[2].set_xlabel('Pixel value')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('(c) True missing values')\n",
    "axes[2].set_xlim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('svhn_histograms.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics about imputations\n",
    "print(\"\\nImputation Statistics:\")\n",
    "print(f\"True missing values - Mean: {np.mean(true_missing_vals):.3f}, Std: {np.std(true_missing_vals):.3f}\")\n",
    "print(f\"MIWAE imputations   - Mean: {np.mean(miwae_imputed_vals):.3f}, Std: {np.std(miwae_imputed_vals):.3f}\")\n",
    "print(f\"not-MIWAE imputations - Mean: {np.mean(notmiwae_imputed_vals):.3f}, Std: {np.std(notmiwae_imputed_vals):.3f}\")\n",
    "\n",
    "# Percentage above clipping threshold\n",
    "print(f\"\\nPercentage above clipping threshold (0.75):\")\n",
    "print(f\"True missing values:  {100*np.mean(true_missing_vals > 0.75):.1f}%\")\n",
    "print(f\"MIWAE imputations:    {100*np.mean(miwae_imputed_vals > 0.75):.1f}%\")\n",
    "print(f\"not-MIWAE imputations: {100*np.mean(notmiwae_imputed_vals > 0.75):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Close Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_miwae.close()\n",
    "sess_notmiwae.close()\n",
    "print(\"Sessions closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notmiwae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
